---
title: "Introduction to exprso"
author: "Thomas Quinn"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

## Introduction

In this series of tutorials, we present the *exprso* package for *R*, a custom library built to tackle a wide variety of supervised machine learning problems, including the construction of ensemble classifiers. We designed this package with the biologist user in mind, carefully assembling a toolkit that allows investigators to quickly implement dichotomous (binary) and multi-class classification on high-dimensional genomic data. We designed *exprso* using a modular framework, whereby each function acts as a self-contained yet interchangable part of the whole. With these modules, the investigator has access to multiple tools that they can use in almost any sequence in order to build their very own personalized machine learning pipeline. By rearranging these modules, one can easily experiment with new pipeline ideas on the fly. In this way, we balance the simplicity of automatation with endless customization, all while maintaining software extensibility.

We have bundled many kinds of functions into the *exprso* package, including functions for data import, data modification, feature selection, individual classifier construction, and ensemble classifier construction. We begin this tutorial by first presenting a brief overview of the objects and functions found in *exprso*. Then, we move on to show how the user can access this package from *GitHub* and import their own data into the *R* environment by one of three different ways.

## exprso Objects

Before exploring the utility of the functions included in this library, we will first overview the principle objects used by this package. Objects in *R* serve as convenient ways of organizing data. One can think of an object as simply a custom built data container. We emphasize these here based on a notion that a better understanding of the back-end promotes a better understanding of the front-end.

* **ExprsArray** Stores feature and annotation data as well as relevant feature selection history.
* **ExprsMachine** Stores the trained machine as well as the relevant feature selection history needed to deploy a machine on an independent dataset.
* **ExprsPipeline** Stores a summary of high-throughput machine performance and the machine objects themselves.
* **ExprsEnsemble** Stores a number of machine objects which function a single ensemble classifier.
* **ExprsPredict** Stores the results of machine prediction as three measures: binary, probability, and decision boundary.

## exprso Functions

The functions included in this library rely on the objects listed above. Some of these functions return an updated version of the same object type provided, while others return a different object type altogether. We have adopted a nomenclature which we hope helps the user organize the functions available in this package. In this scheme, most functions have a few letters in the beginning of their name which designate their general utility. We include a flow diagram of available functions to go along with the brief description of function prefixes:

* **array** Any function that has a role in importing or subsetting data. Returns an updated ExprsArray object.
* **mod** Any function that has a role in pre-processing ExprsArray objects. Returns an updated ExprsArray object.
* **split** Any function that has a role in splitting ExprsArray objects into training and test sets. Returns a list of two ExprsArray objects.
* **fs** Any function that has a role in feature selection. Returns an updated ExprsArray object.
* **build** Any function that has a role in building a classifier or classifier ensemble. Returns an ExprsMachine or ExprsEnsemble object.
* **pl** Any function that has a role in high-throughput analytics. Usually returns an ExprsPipeline object.
* **pipe** Any function that has a role in post-processing ExprsPipeline objects. Returns an updated ExprsPipeline object.

## Importing the library

We can load the most recent version of *exprso* directly from *GitHub* using the `install_github` function provided by the *devtools* package.

```{r, eval = FALSE}
library(devtools)
devtools::install_github("tpq/exprso")
```

```{r, message = FALSE}
library(exprso)
```

## Importing data

With *exprso* loaded, we can import data in one of three ways. First, we can import data as stored in a tab-delimited text file. The construction of this import function, `arrayRead`, assumes the data file contains the subjects as rows and variables as columns, with annotation variables (e.g. sex or age) appearing before the feature variables (e.g. mRNA expression values). The `include` argument lists any number of annotation vectors to assign to each class label. In the example below, we assign the annotation "Control" to the first class, then we assign the annotations "ASD" and "PDDNOS" to the second class. Any subjects not assigend to a class will get dropped from the resultant object. The user can find more details in the function specific documentation.

```{r, eval = FALSE}
array <-
  arrayRead("some_example_data_file.txt", # tab-delimited file
            probes.begin = 11, # the i-th column where features begin
            colID = "Subject.ID", # column name with subject ID
            colBy = "DX", # column name with class labels
            include = list(c("Control"), c("ASD", "PDDNOS")))
```

Second, we can import data into *exprso* directly by converting an *eSet* object, a popular container for storing gene expression data, using the function `arrayEset`. The user can find more details in the function specific documentation. In the example below, we convert the classic ALL/AML Golub gene expression dataset from an eSet object (provided by the *golubEsets* package) into an ExprsArray object. We will use these data routinely for the remainder of this tutorial. We note here that GDS and GSE data files (from the NCBI GEO repository) easily convert to eSet objects using the `GDS2eSet` and `GSE2eSet` functions from the *Biobase* and *exprso* packages, respectively.

```{r, message = FALSE}
library(golubEsets)
data(Golub_Merge)
array <-
  arrayEset(Golub_Merge, # an ExpressionSet (abrv. eSet) object
            colBy = "ALL.AML", # column name with class labels
            include = list("AML", "ALL"))
```

Third, we can construct an ExprsArray object manually using the `new` function. To accomplish this, we need to define the four slots found which comprise an ExprsArray object. The `@exprs` slot must contain a feature value `matrix` with subject names as columns. The `@annot` slot must contain an annotation `data.frame` with subject names as rows. It is critical to make sure that the subject names appear in the same order across both data structures. The two additional slots, `@preFilter` and `@reductionModel`, pass along the feature selection history while deploying built machines. When constructing an ExprsArray object manually, set both of these to `NULL`.

```{r, eval = FALSE}
array <-
  new("ExprsArray",
      exprs = some.expression.matrix,
      annot = some.annotation.data.frame,
      preFilter = NULL,
      reductionModel = NULL)
```

## Subsetting data

[PLACEHOLDER]

## Splitting data

When performing classification, an investigator will typically withhold a percentage of the data to in order to assess classifier performance, effectively splitting the data into two. The first dataset, called the *training set*, gets used to train the model, while the other, called the external validation or *test set*, gets used to evaluate the model. This package offers two convenience functions for splitting the data, `splitSample` and `splitStratify`. The former builds the training set based on simple random sampling (with or without replacement), assigning the remaining subjects to the test set. The latter builds the training set using stratified random sampling. These functions both return a list of two ExprsArray objects corresponding to the training set and test set respectively. Below, we use the `splitStratify` function to build the training and test sets through a stratified random sample across the dichotomous (binary) classification annotation.

```{r}
arrays <-
  splitStratify(array,
                percent.include = 67,
                colBy = NULL)

array.train <- arrays[[1]]
```

## Balancing data

Above, all subjects not included in the training set (based on `percent.include`) will automatically get assigned to the test set. When using `splitStratify` on a dataset with an unequal number of annotated subjects (e.g. more cases than controls), the resultant test set may contain relative annotation frequencies that differ from the training set. We can fix this so-called "imbalance" at the cost of reducing sample size by performing `splitStratify` a second time. Now, we will use the test set as the input and let `percent.include = 100` (keeping the other parameters the same). This will split the test set such that the new "training set" (i.e. slot 1) now contains the *balanced test set* and the new "test set" (i.e. slot 2) now contains the "spillover" due to balancing.

```{r}
balance <-
  splitStratify(arrays[[2]],
                percent.include = 100,
                colBy = NULL)

array.test <- balance[[1]]
```

## Selecting features

Considering the high-dimensionality of most genomic datasets, it is prudent and often necessary to prioritize which features to include during classifier construction. There exists a myriad of ways to perform the task of *feature selection*. This package provides functions for some of the most frequently used feature selection methods. Each function works as a self-contained wrapper that (1) pre-processes the ExprsArray input, (2) performs the feature selection, and (3) returns an ExprsArray output with an updated feature selection history. The user may deploy, in tandem, any number of these functions in whatever order they choose, limited only by computational power and imagination.

The feature selection histories get passed along at every step of the way until they eventually get used in order to pre-process an unlabelled dataset during classifier deployment (i.e. prediction). In the spirit of open source programming, we encourage users to submit their own feature selection functions, modeled after those provided in this library.

The first of these feature selection functions, `fsStats`, performs some of the most basic feature selection methods: those rooted in simple statistical tests. Specifically, this function will rank probes based on either the Student's *t*-test or the Kolmogorov-Smirnov test. Below, we rank features according to the results of a Student's *t*-test.

```{r}
array.train <-
  fsStats(array.train, probes = 0, how = "t.test")
```

On first exposure, we acknowledge that the argument `probes` might seem confusing. Therefore, we wish to emphasize the role of this term in specifying either the names or the number of features to supply *to* feature selection: it does not refer to what the user intends to retrieve *from* the feature selection process. When calling the first feature selection method with a numeric *probes* argument, it will draw from the "top ranked" probe set that actually refers to the default order in which the features appear in the ExprsArray input. However, each successful feature selection will return an ExprsArray output with the features implicitly (re-)ranked. Therefore, when calling a subsequent feature selection method, the "top ranked" probe set will indeed correspond to the feature ranks from the previous feature selection process. For example, the third feature selection call draws the top probes from the second feature ranking.

Another included feature selection method, `fsPrcomp`, performs dimension reduction by way of principal components analysis (**PCA**). The dimension reduction model generated during this function call gets saved in the feature selection history along with the compontents themselves. Below, we subject the top 50 probes as prioritized by `fsStats` to principal components analysis.

```{r}
array.train <-
  fsPrcomp(array.train, probes = 50)
```

The other feature selection methods included in this package all follow the same use pattern. Below, we plot the first three components of the training set in 3-dimensional space.

```{r}
plot(array.train)
```

## Constructing classifiers

This package exposes several methods for supervised machine learning, including wrapper functions that implement support vector machines, artificial neural networks, random forests, and more. These functions require an ExprsArray object as input and return a single ExprsModel object as output. This ExprsModel object contains the feature selection history that led up to classifier construction as well as the classifier itself. Below, we build an artificial neural network with five intermediate nodes using the top 10 components from the *training set* above.

```{r}
mach <-
  buildANN(array.train, probes = 10, size = 5)
```

## Deploying classifiers

We deploy an ExprsModel object using `predict`. This function returns an ExprsPredict object containing the prediction results in three forms: prediction, probability, and decision boundary predictions. The probability and decision boundary predictions relate to one another by a logistic transformation. The prediction (`@pred`) slot converts these metrics into a single "all-or-nothing" class label assignment.

Another function, `calcStats`, allows us to compare the prediction results (as stored within an ExprsPredict object) against the actual diagnostic labels found in the corresponding ExprsArray object. The `aucSkip` argument specifies whether to calculate the area under the receiver operating characteristic (**ROC**) curve.

We note here that performance metrics calculated using the ROC may differ from those calculated using a confusion matrix because the former may adjust the discrimination threshold to optimize sensitivity and specificity. The discrimination threshold is automatically chosen as the point along the ROC which minimizes the Euclidean distance from (0, 1). Below, we deploy a classifier on the *test set*, then use the result to calculate classifier performance.

```{r}
pred <-
  predict(mach, array.test)
```

```{r}
calcStats(pred, array.test)
```

## Pipeline methods

This package includes several functions named with the prefix `pl`. These "pipeline" functions exist to help consolidate high-throughput analyses. In other words, they wrap repetitive tasks in a single operation. Such tasks include high-throughput parameter searches as well as elaborate cross-validation designs. As we will see, some of these `pl` functions will even have other `pl` functions embedded within them. For example, the function `plGrid` contains the function `plCV` which manages *v*-fold and leave-one-out cross-validation.

## High-throughput parameter searches

When constructing a classifier using a **build** method, we can only specify one set of parameters at a time. However, we often want to test models across a vast range of parameters. For this task, we provide the `plGrid` function. This function builds and deploys a model for combination of all provided arguments. For example, given `how = "buildSVM"` with the arguments `probes = c(3, 5, 10)`, `cost = 10^(-3:3)`, and `kernel = c("linear", "radial")`, this function will build (and deploy) 42 machines.

This function does accept only one `how` per run. To analyze the results of multiple `build` parameter searches jointly, combine the results of those `plGrid` function calls using `?conjoin`. We also note here that `plGrid` does not handle data splitting or feature selection, both of which the user may perform beforehand. However, it does allow the user to specify multiple *classifier sizes* using the `probes` argument.

The `plGrid` function can also calculate *v*-fold cross-validation accuracy at each step of the parameter search (toggled by supplying a non-`NULL` argument to `fold`). We emphasize, however, that the cross-validation method embedded within `plGrid` (i.e., `plCV`) does not re-select features with each fold, which may lead to overly-optimistic measures of classifier performance in the setting of prior feature selection.

Below, we will run through a few different support vector machine (SVM) builds, calculating leave-one-out cross-validation accuracy (by setting `fold = 0`) at each step.

```{r, results = "hide", warning = FALSE}
gs <-
  plGrid(array.train = array.train,
         array.valid = array.test,
         probes = 0,
         how = "buildSVM",
         fold = 0,
         kernel = "linear",
         cost = 10^(-3:3)
)
```

The returned object contains two slots, `@summary` and `@machs`, which store the performance summary and corresponding `ExprsModel` objects, respectively. The performance summary contains columns detailing the parameter used to build each machine along with performance metrics for the training set (and test set, if provided). Columns named with "train" describe training set performances. Columns named with "valid" describe test set performances. The column, `"train.plCV"`, contains the cross-validation accuracy, if performed. The returned `ExprsPipeline` object also contains an `ExprsModel` for each entry in the performance summary.

We can subset `ExprsPipeline` objects by the performance summary using the `[` and `$` operators, similar to how `ExprsArray` subsetting works. Below, we extract the column containing the cross-validation accuracies.

```{r}
gs[, "train.plCV"]
```

```{r}
gs$train.plCV
```

## Other cross-validation

The *exprso* package also provides a means by which to perform elaborate cross-validation, including Monte Carlo style and 2-layer "nested" cross-validation. Analogous to how `plGrid` manages multiple build and predict tasks, these elaborate cross-validation pipelines (`plMonteCarlo` and `plNested`) effectively manage multiple `plGrid` tasks. In order to organize the sheer number of arguments necessary to execute these kinds of cross-validation analyses, we have implemented argument handler functions. These argument handler functions, `ctrlSplitSet`, `ctrlFeatureSelect`, and `ctrlGridSearch` manage data splitting, feature selection, and grid searching, respectively.

In simplest terms, both of these cross-validation methods use a single training set to calculate classifier performances on a withheld validation set. This *internal* validation set serves as a kind of proxy for a statistically independent test set. The main difference between `plMonteCarlo` and `plNested` stems from how *internal validation set* gets constructed. On one hand, the `plMonteCarlo` method uses the `ctrlSplitSet` argument handler to split the training set into a *training subset* and an *intenral validation set* with each boostrap. On the other hand, the `plNested` method splits the training set into *v*-folds, treating each fold as an *internal validation set*, with the remaining subjects comprising the *training subset*.

For simplicity, we can call any performance measured on an *internal validation set* as the **outer-loop** cross-validation performance. Then, we can any cross-validation accuracy measured using the *training subset* (i.e., via `plGrid`) the **inner-loop** cross-validation performance. In the performance summaries of the `ExprsPipeline` objects returned by `plMonteCarlo` and `plNested`, columns named with "train" describe *training subset* performances while columns named with "valid" describe *internal validation set* performances.

Although the **inner-loop** cross-validation performances (i.e., via `plCV`) can over-estimate cross-validation through prior feature selection, the **outer-loop** cross-validation performances derive from classifiers that have undergone feature selection anew with each bootstrap or fold. However, we emphasize here that performing feature selection on a training set *prior* to the use of `plMonteCarlo` or `plNested` can still result in overly optimistic **outer-loop** cross-validation performances.

In the example below, we perform five iterations of `plMonteCarlo` using the original training set before it underwent any feature selection (i.e., the first slot of the object `balance`). With each iteration, this function will (a) sample the subjects randomly through bagging (i.e., random sampling with replacement), (b) perform feature selection using the Student's t-test, then (c) execute a gridsearch across multiple support vector machine (SVM) parameters and classifier sizes.

```{r}
ss <-
  ctrlSplitSet(func = "splitSample", percent.include = 67, replace = TRUE)
fs <-
  ctrlFeatureSelect(func = "fsStats", probes = 0, how = "t.test")
gs <-
  ctrlGridSearch(func = "plGrid",
                 how = "buildSVM",
                 probes = c(10, 25, 50),
                 kernel = "linear",
                 cost = 10^(-3:3),
                 fold = 10)
```

```{r, results = "hide", warning = FALSE}
boot <-
  plMonteCarlo(balance[[1]],
               B = 5,
               ctrlSS = ss,
               ctrlFS = fs,
               ctrlGS = gs)
```

To perform multiple feature selection tasks with each bootstrap or fold, supply a list of multiple `ctrlFeatureSelect` argument wrappers to the `ctrlFS` argument. To reduce the results of `plMonteCarlo` or `plNested` to a single performance metric, feed the returned ExprsPipeline object through `calcMonteCarlo` or `calcNested`, respectively.

```{r}
calcMonteCarlo(boot, colBy = "valid.auc")
```

## Ensemble classifiers

This package provides two ways to build ensemble classifiers. The first involves manually combining multiple `ExprsModel` objects together through the function `buildEnsemble` (or `conjoin`). The second involves an orchestrated manipulation of an `ExprsPipeline` object through the `pipeFilter` method.

This `pipeFilter` method filters an `ExprsPipeline` object in (up to) three steps. First, a threshold filter gets imposed. Any model with a performance less than the threshold filter, `how`, gets excluded. Second, a ceiling filter gets imposed. Any model with a performance less than the ceiling filter, `gate`, gets excluded. Third, an arbitrary subset occurs. The top N models in the `ExprsPipeline` object get selected based on the argument `top.N`. However, in the case that the `@summary` slot contains the column "boot" (e.g., in the results of `plMonteCarlo`), `pipeFilter` selects the top N models for *each* unique bootstrap. However, the user may skip any one of these filter steps by setting the respective argument to 0.

When calling `buildEnsemble`, any classifiers remaining after the `pipeFilter` filter will get assembled into a single ensemble classifier. Ensemble classifiers get stored as an `ExprsEnsemble` object which is nothing more than object-oriented container for a list of `ExprsModel` objects.

In the example below, we we will build an ensemble using the single best classifier from each `plMonteCarlo` bootstrap, and then deploy that ensemble on the withheld test set (i.e., the second slot of the object `balance`).

```{r}
ens <- buildEnsemble(boot, top.N = 1, colBy = "valid.auc")
```

```{r}
predict(ens, array.test, how = "probability")
```

Owing to how `pipeFilter` handles `ExprsPipeline` objects which contain a "boot" column in the performance summary, we include the `pipeUnboot` function to rename this "boot" column to "unboot". Otherwise, to learn more about how `ExprsEnsemble` predicts class labels, see `?'exprso-predict'`. In addition, we encourage the user to visit the package documentation, `?'ExprsPipeline-class'` and `?'ExprsEnsemble-class'`.

## Multi-class methods

We conclude this vignette by altering the user that the *exprso* package also includes a framework for performing multi-class classification in an automated manner. These methods use the "1-vs-all" approach to multi-class classification, whereby each individual class label has a turn getting treated as the positive class label in a dichotomous (binary) framework. Then, the results of each iteration get integrated into a single construct. To learn more about multi-class classification, we refer the user to the package documentation, `?arrayRead` and `?doMulti`. However, we should emphasize here that multi-class feature selection is still under heavy development. In the near future, we plan to introduce novel `pl` methods that implement multi-class feature selection in an optimal manner.

## Final remarks

Thank you for your interest in *exprso*. Although we have made tremendous progress in formalizing this package under a reliable framework, some of the tools included here may eventually change. We will make our best effort to keep this vignette and other documentation as up-to-date as possible. However, we may occasionally fall short on this goal. In face of any discrepancy, we encourage the reader to send any and all questions to the package maintainer (e-mail: thom@tpq.me). Under some circumstances, we even welcome requests for new tools that we could bundle into future installments.
