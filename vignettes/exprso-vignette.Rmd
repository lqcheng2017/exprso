---
title: "Introduction to exprso"
author: "Thomas Quinn"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

## Introduction

In this series of tutorials, we present the *exprso* package for *R*, a custom library built to tackle a wide variety of supervised machine learning problems, including the construction of ensemble classifiers. We designed this package with the biologist user in mind, carefully assembling a toolkit that allows investigators to quickly implement dichotomous (binary) and multi-class classification on high-dimensional genomic data. We designed *exprso* using a modular framework, whereby each function acts as a self-contained yet interchangable part of the whole. With these modules, the investigator has access to multiple tools that they can use in almost any sequence in order to build their very own personalized machine learning pipeline. By rearranging these modules, one can easily experiment with new pipeline ideas on the fly. In this way, we balance the simplicity of automatation with endless customization, all while maintaining software extensibility.

We have bundled many kinds of functions into the *exprso* package, including functions for data import, data modification, feature selection, individual classifier construction, and ensemble classifier construction. We begin this tutorial by first presenting a brief overview of the objects and functions found in *exprso*. Then, we move on to show how the user can access this package from *GitHub* and import their own data into the *R* environment by one of three different ways.

## exprso Objects

Before exploring the utility of the functions included in this library, we will first overview the principle objects used by this package. Objects in *R* serve as convenient ways of organizing data. One can think of an object as simply a custom built data container. We emphasize these here based on a notion that a better understanding of the back-end promotes a better understanding of the front-end.

* **ExprsArray** Stores feature and annotation data as well as relevant feature selection history.
* **ExprsMachine** Stores the trained machine as well as the relevant feature selection history needed to deploy a machine on an independent dataset.
* **ExprsPipeline** Stores a summary of high-throughput machine performance and the machine objects themselves.
* **ExprsEnsemble** Stores a number of machine objects which function a single ensemble classifier.
* **ExprsPredict** Stores the results of machine prediction as three measures: binary, probability, and decision boundary.

## exprso Functions

The functions included in this library rely on the objects listed above. Some of these functions return an updated version of the same object type provided, while others return a different object type altogether. We have adopted a nomenclature which we hope helps the user organize the functions available in this package. In this scheme, most functions have a few letters in the beginning of their name which designate their general utility. We include a flow diagram of available functions to go along with the brief description of function prefixes:

* **array** Any function that has a role in importing or subsetting data. Returns an updated ExprsArray object.
* **mod** Any function that has a role in pre-processing ExprsArray objects. Returns an updated ExprsArray object.
* **split** Any function that has a role in splitting ExprsArray objects into training and test sets. Returns a list of two ExprsArray objects.
* **fs** Any function that has a role in feature selection. Returns an updated ExprsArray object.
* **build** Any function that has a role in building a classifier or classifier ensemble. Returns an ExprsMachine or ExprsEnsemble object.
* **pl** Any function that has a role in high-throughput analytics. Usually returns an ExprsPipeline object.
* **pipe** Any function that has a role in post-processing ExprsPipeline objects. Returns an updated ExprsPipeline object.

## Importing the library

We can load the most recent version of *exprso* directly from *GitHub* using the `install_github` function provided by the *devtools* package.

```{r, eval = FALSE}
library(devtools)
devtools::install_github("tpq/exprso")
```

```{r}
library(exprso)
```

## Importing data

With *exprso* loaded, we can import data in one of three ways. First, we can import data as stored in a tab-delimited text file. The construction of this import function, `arrayRead`, assumes the data file contains the subjects as rows and variables as columns, with annotation variables (e.g. sex or age) appearing before the feature variables (e.g. mRNA expression values). The `include` argument lists any number of annotation vectors to assign to each class label. In the example below, we assign the annotation "Control" to the first class, then we assign the annotations "ASD" and "PDDNOS" to the second class. Any subjects not assigend to a class will get dropped from the resultant object. The user can find more details in the function specific documentation.

```{r, eval = FALSE}
array <-
  arrayRead("some_example_data_file.txt", # tab-delimited file
            probes.begin = 11, # the i-th column where features begin
            colID = "Subject.ID", # column name with subject ID
            colBy = "DX", # column name with class labels
            include = list(c("Control"), c("ASD", "PDDNOS")))
```

Second, we can import data into *exprso* directly by converting an *eSet* object, a popular container for storing gene expression data, using the function `arrayEset`. The user can find more details in the function specific documentation. In the example below, we convert the classic ALL/AML Golub gene expression dataset from an eSet object (provided by the *golubEsets* package) into an ExprsArray object. We will use these data routinely for the remainder of this tutorial. We note here that GDS and GSE data files (from the NCBI GEO repository) easily convert to eSet objects using the `GDS2eSet` and `GSE2eSet` functions from the *Biobase* and *exprso* packages, respectively.

```{r}
library(golubEsets)
data(Golub_Merge)
array <-
  arrayEset(Golub_Merge, # an ExpressionSet (abrv. eSet) object
            colBy = "ALL.AML", # column name with class labels
            include = list("AML", "ALL"))
```

Third, we can construct an ExprsArray object manually using the `new` function. To accomplish this, we need to define the four slots found which comprise an ExprsArray object. The `@exprs` slot must contain a feature value `matrix` with subject names as columns. The `@annot` slot must contain an annotation `data.frame` with subject names as rows. It is critical to make sure that the subject names appear in the same order across both data structures. The two additional slots, `@preFilter` and `@reductionModel`, pass along the feature selection history while deploying built machines. When constructing an ExprsArray object manually, set both of these to `NULL`.

```{r, eval = FALSE}
array <-
  new("ExprsArray",
      exprs = some.expression.matrix,
      annot = some.annotation.data.frame,
      preFilter = NULL,
      reductionModel = NULL)
```

## Subsetting

[PLACEHOLDER]

## Splitting

When performing classification, an investigator will typically withhold a percentage of the data to in order to assess classifier performance, effectively splitting the data into two. The first dataset, called the *training set*, gets used to train the model, while the other, called the external validation or *test set*, gets used to evaluate the model. This package offers two convenience functions for splitting the data, `splitSample` and `splitStratify`. The former builds the training set based on simple random sampling (with or without replacement), assigning the remaining subjects to the test set. The latter builds the training set using stratified random sampling. These functions both return a list of two ExprsArray objects corresponding to the training set and test set respectively. Below, we use the `splitStratify` function to build the training and test sets through a stratified random sample across the dichotomous (binary) classification annotation.

```{r, eval = FALSE}
arrays <-
  splitStrat(array,
             percent.include = 67,
             colBy = NULL)

array.train <- arrays[[1]]
```

## Balancing

Above, all subjects not included in the training set (based on `percent.include`) will automatically get assigned to the test set. When using `splitStratify` on a dataset with an unequal number of annotated subjects (e.g. more cases than controls), the resultant test set may contain relative annotation frequencies that differ from the training set. We can fix this so-called "imbalance" at the cost of reducing sample size by performing `splitStratify` a second time. Now, we will use the test set as the input and let `percent.include = 100` (keeping the other parameters the same). This will split the test set such that the new "training set" (i.e. slot 1) now contains the *balanced test set* and the new "test set" (i.e. slot 2) now contains the "spillover" due to balancing.

```{r}
balance <-
  splitStrat(arrays[[2]],
             percent.include = 100,
             colBy = NULL)

array.test <- balance[[1]]
```

